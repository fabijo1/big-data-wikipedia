{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import wikipedia\n",
    "from pyspark.ml.feature import CountVectorizer, RegexTokenizer, StopWordsRemover\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SQLContext\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "#LocalMode\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#StandaloneMode\n",
    "#spark = SparkSession.builder.master('spark://localhost:7077').getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "def get_title(content):\n",
    "    # Remove any leading or lagging space if present \n",
    "    content = content.strip()\n",
    "    title = ''\n",
    "    try:\n",
    "        if(content != ''):\n",
    "            # Split the content on the basis of new line\n",
    "            arr = content.split('\\n', 2)\n",
    "            # Second line is the title\n",
    "            title = arr[1]\n",
    "            # Rest is the actual content\n",
    "            actual_content = arr[2]\n",
    "    except:\n",
    "        title = 'error'\n",
    "    return title\n",
    "\n",
    "def get_content(content):\n",
    "    # Remove any leading or lagging space if present \n",
    "    content = content.strip()\n",
    "    actual_content = ''\n",
    "    try:\n",
    "        if(content != ''):\n",
    "            # Split the content on the basis of new line\n",
    "            arr = content.split('\\n', 2)\n",
    "            # Second line is the title\n",
    "            title = arr[1]\n",
    "            # Rest is the actual content\n",
    "            actual_content = arr[2]\n",
    "    except:\n",
    "        actual_content = 'error'\n",
    "    return actual_content\n",
    "\n",
    "def clean(article):\n",
    "    title = article[0]\n",
    "    document = article[1]\n",
    "    tokens = RegexpTokenizer(r'\\w+').tokenize(document.lower())\n",
    "    tokens_clean = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    tokens_stemmed = [PorterStemmer().stem(token) for token in tokens_clean]\n",
    "    return (title, tokens_stemmed)\n",
    "\n",
    "def splitByDoc(textfile):\n",
    "    return list(filter(lambda x: x != '\\n', textfile[1].split('</doc>')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Alina-PC:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x17df6d9deb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.319759130477905\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "data = sc.wholeTextFiles('C:/Users/Alina/Big Data/Wikipedia Exports/all_articles_2mb/*/*')\n",
    "pagesRaw = data.flatMap(splitByDoc)\n",
    "pagesTitleContent = pagesRaw.map(lambda x: (get_title(x), get_content(x))).filter(lambda x: x[0] != 'error' and x[0] != '')\n",
    "\n",
    "#RDD to DataFrame\n",
    "dfPagesTitleContent = sqlContext.createDataFrame(pagesTitleContent, ['title', 'content'])\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol='content', outputCol='list_of_words_raw', pattern='\\\\W', minTokenLength=4)\n",
    "\n",
    "stopWordsRemover = StopWordsRemover(inputCol='list_of_words_raw', outputCol='list_of_words')\n",
    "stopwordsSpark = stopWordsRemover.getStopWords()\n",
    "stopwordsSpark.extend(['also'])\n",
    "stopWordsRemover.setStopWords(stopwordsSpark)\n",
    "\n",
    "countVectorizer = CountVectorizer(inputCol='list_of_words', outputCol='features')\n",
    "\n",
    "lda = LDA(k=20, maxIter=15)\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopWordsRemover, countVectorizer, lda])\n",
    "model = pipeline.fit(dfPagesTitleContent)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RegexTokenizer_6456ea0e87c3,\n",
       " StopWordsRemover_50d1df041045,\n",
       " CountVectorizerModel: uid=CountVectorizer_713da8535bfd, vocabularySize=24678,\n",
       " LocalLDAModel: uid=LDA_8e8443e7507e, k=20, numFeatures=24678]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anarchism\\r',\n",
       " 'Autism\\r',\n",
       " 'Albedo\\r',\n",
       " 'A\\r',\n",
       " 'Alabama\\r',\n",
       " 'Achilles\\r',\n",
       " 'Abraham Lincoln\\r',\n",
       " 'Aristotle\\r',\n",
       " 'An American in Paris\\r',\n",
       " 'Academy Award for Best Production Design\\r',\n",
       " 'Academy Awards\\r',\n",
       " 'Actrius\\r',\n",
       " 'Animalia (book)\\r',\n",
       " 'International Atomic Time\\r',\n",
       " 'Altruism\\r',\n",
       " 'Ayn Rand\\r',\n",
       " 'Alain Connes\\r',\n",
       " 'Allan Dwan\\r',\n",
       " 'Algeria\\r',\n",
       " 'List of Atlas Shrugged characters\\r']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedPagesTitles = pagesTitleContent.map(lambda x: x[0])\n",
    "cleanedPagesTitles.take(20)\n",
    "#cleanedPagesTitles.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['connes', 'sciences', 'academy', 'algebras', 'geometry', 'medal', 'operator', 'theory', 'aardwolf', 'differential'], ['loimios', 'pigmentation', 'american', 'caliber', 'generalization', 'biogas', 'ludwig', 'pseudonyms', 'carbocation', 'overlaps'], ['used', 'letter', 'alphabet', 'english', 'many', 'form', 'greek', 'variants', 'latin', 'cursive'], ['achilles', 'hector', 'thetis', 'patroclus', 'troy', 'odysseus', 'zeus', 'agamemnon', 'ajax', 'iliad'], ['used', 'letter', 'alphabet', 'ambiguity', 'form', 'ambiguous', 'english', 'variants', 'languages', 'cursive'], ['huxley', 'bates', 'song', 'america', 'beautiful', 'poem', 'published', 'ward', 'first', 'written'], ['ketchikan', 'famed', 'orange', 'juniper', 'collectives', 'santee', 'manually', 'mixtures', 'conducts', 'voices'], ['congress', 'states', 'articles', 'confederation', 'constitution', 'state', 'continental', 'government', 'union', 'power'], ['alaska', 'andorra', 'state', 'first', 'lincoln', 'time', 'states', 'used', 'many', 'century'], ['oxymoron', 'causal', 'venezuela', 'missing', 'materialist', 'amphibiorum', 'cassander', 'derivations', 'guides', 'hotter'], ['aardwolf', 'termites', 'aardwolves', 'africa', 'night', 'hyena', 'family', 'territory', 'dens', 'season'], ['court', 'appeal', 'states', 'trial', 'appellate', 'articles', 'lower', 'state', 'congress', 'review'], ['threw', 'linkages', 'italic', 'retorted', 'leyden', 'unoccupied', 'enthalpy', 'snowmelt', 'herrick', 'reply'], ['anthropology', 'anthropologists', 'sociocultural', 'ethnographic', 'anthropological', 'soci', 'genetics', 'dissipate', 'transpersonal', 'counterinsurgency'], ['states', 'congress', 'articles', 'confederation', 'constitution', 'state', 'government', 'song', 'first', 'america'], ['repeating', 'london', 'afrasian', 'trademark', 'shawiya', 'hubris', 'pauline', 'auscultation', 'spectacle', 'organized'], ['trafficking', '1798', 'alice', 'specializing', 'conviction', 'dangerous', 'adoption', 'challenging', 'racism', 'phosphoric'], ['ceasing', 'newsweek', 'unmistakably', 'kelp', 'evenly', 'hermes', 'producers', 'tibet', 'breaker', 'turnabout'], ['achilles', 'ambiguity', 'aardvark', 'example', 'acid', 'aardwolf', 'anatomy', 'adventure', 'lewis', 'termites'], ['reunited', 'artimus', 'prepared', 'crystallisation', 'pesticides', 'apollonius', 'profusely', 'ascribes', 'prosthetics', 'aeronautics']]\n"
     ]
    }
   ],
   "source": [
    "vocab = model.stages[2].vocabulary\n",
    "topics = model.stages[3].describeTopics()\n",
    "topicsRdd = topics.rdd\n",
    "topicsRaw = topicsRdd.map(lambda row: row['termIndices']).collect()\n",
    "result = map(lambda entry: [vocab[idx] for idx in entry], topicsRaw)\n",
    "print(list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plan',\n",
       " 'typic',\n",
       " 'diagram',\n",
       " 'list',\n",
       " 'step',\n",
       " 'detail',\n",
       " 'time',\n",
       " 'resourc',\n",
       " 'use',\n",
       " 'achiev',\n",
       " 'object',\n",
       " 'someth',\n",
       " 'commonli',\n",
       " 'understood',\n",
       " 'tempor',\n",
       " 'set',\n",
       " 'intend',\n",
       " 'action',\n",
       " 'one',\n",
       " 'expect']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_title = \"Plant\"\n",
    "article_content_test = clean([article_title, wikipedia.page(article_title).content])[1]\n",
    "article_content_test_rdd = sc.parallelize([article_content_test]).zipWithIndex()\n",
    "df_txts_test  = sqlContext.createDataFrame(article_content_test_rdd, ['list_of_words', 'index'])\n",
    "cv_test = CountVectorizer(inputCol='list_of_words', outputCol='features')\n",
    "\n",
    "cvmodel_test = cv_test.fit(df_txts_test)\n",
    "result_cv_test = cvmodel_test.transform(df_txts_test)\n",
    "\n",
    "result_cv_test.select('list_of_words').rdd.flatMap(list).flatMap(list).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower bound on the log likelihood of the entire corpus: -687484.6794305448\n",
      "The upper bound on perplexity: 1617.6110104248114\n"
     ]
    }
   ],
   "source": [
    "ll = model.stages[3].logLikelihood(result_cv_test)\n",
    "lp = model.stages[3].logPerplexity(result_cv_test)\n",
    "print('The lower bound on the log likelihood of the entire corpus: ' + str(ll))\n",
    "print('The upper bound on perplexity: ' + str(lp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
